{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "170ec4e3-8f52-4960-891a-9dc50889fa7c",
   "metadata": {},
   "source": [
    "##### Tokenize words and sentences using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e61f04c3-b691-41d9-bedc-013e30c601e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e020655c-d744-411b-b38f-c86d7425adf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = '''\n",
    "    Good muffins cost $3.88\\nin New York. Please buy me two of them.\\n\\nThanks.\n",
    "    '''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59c6dc7-d047-447b-8b6e-b73fcd7d0f35",
   "metadata": {},
   "source": [
    "word_tokenize splits the text into a list of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08eeb170-e960-42a2-bb46-b12edab74c8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Good',\n",
       " 'muffins',\n",
       " 'cost',\n",
       " '$',\n",
       " '3.88',\n",
       " 'in',\n",
       " 'New',\n",
       " 'York',\n",
       " '.',\n",
       " 'Please',\n",
       " 'buy',\n",
       " 'me',\n",
       " 'two',\n",
       " 'of',\n",
       " 'them',\n",
       " '.',\n",
       " 'Thanks',\n",
       " '.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a321fb4a-3e8f-4c4c-a16a-d1e8312fcbac",
   "metadata": {},
   "source": [
    "sent_tokenize separates the text by _sent_ence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6078dfae-aa6a-4fe2-a258-172b6da97480",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n    Good muffins cost $3.88\\nin New York.',\n",
       " 'Please buy me two of them.',\n",
       " 'Thanks.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7274209b-9e43-4a65-bccc-c096934007eb",
   "metadata": {},
   "source": [
    "##### Tokenize words and sentences using Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a5be70e-7172-480e-9bd8-943d38f908e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21ed1f54-ab50-4841-8406-ab4081d83a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4443dcf1-35cd-4f67-b1ae-28b9231fb73a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n    ',\n",
       " 'Good',\n",
       " 'muffins',\n",
       " 'cost',\n",
       " '$',\n",
       " '3.88',\n",
       " '\\n',\n",
       " 'in',\n",
       " 'New',\n",
       " 'York',\n",
       " '.',\n",
       " 'Please',\n",
       " 'buy',\n",
       " 'me',\n",
       " 'two',\n",
       " 'of',\n",
       " 'them',\n",
       " '.',\n",
       " '\\n\\n',\n",
       " 'Thanks',\n",
       " '.',\n",
       " '\\n    ']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#using list comprehension to tokenize words\n",
    "[token.text for token in doc]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cddbbd51-73c7-4ae8-af03-ef27972aa2d0",
   "metadata": {},
   "source": [
    "#### NB Errata in book\n",
    "original code has 'sent.string.strip()' but the text attribure is needed instead as per [stackoverflow](https://stackoverflow.com/questions/67646070/attributeerror-spacy-tokens-span-span-object-has-no-attribute-string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "674d07d9-30c8-4529-9484-ad8f4a482be8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Good muffins cost $3.88\\nin New York.',\n",
       " 'Please buy me two of them.',\n",
       " '',\n",
       " 'Thanks.',\n",
       " '']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[sent.text.strip() for sent in doc.sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a52549e3-7d72-458a-b491-c1859c9d13b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator at 0x144021d60>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.sents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdcfc861-ee33-4097-98bb-f36b5674c332",
   "metadata": {},
   "source": [
    "#### Stemming\n",
    "Stemming is not use so much today in NLP applications, it just chops words down and doesn't consider context.<br>\n",
    "it looks as if lemmatization may be better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "88811ffb-7536-4c9b-9a45-84e7e729f1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3e10533f-446a-4736-b37a-fc556a80f3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['caresses', 'flies', 'dies', 'mules', 'denied',\n",
    "    'died', 'agreed', 'owned', 'humbled', 'sized',\n",
    "    'meetings', 'stating', 'siezing', 'itemization',\n",
    "    'sensational', 'traditional', 'reference', 'colonizer',\n",
    "    'plotted']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a029b757-1608-4ee2-b165-e5fe2f5edae5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['caress',\n",
       " 'fli',\n",
       " 'die',\n",
       " 'mule',\n",
       " 'deni',\n",
       " 'die',\n",
       " 'agre',\n",
       " 'own',\n",
       " 'humbl',\n",
       " 'size',\n",
       " 'meet',\n",
       " 'state',\n",
       " 'siez',\n",
       " 'item',\n",
       " 'sensat',\n",
       " 'tradit',\n",
       " 'refer',\n",
       " 'colon',\n",
       " 'plot']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[stemmer.stem(word) for word in words]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da41c796-9f70-4ee0-b4fa-c5182241c043",
   "metadata": {},
   "source": [
    "#### Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4266b850-e7a8-4f3d-a20b-3790eca57a4b",
   "metadata": {},
   "source": [
    "Lemmatize using nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "85fc0f36-b716-4b53-8109-071f76e890e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6765758c-0b8d-4a72-bec9-4dfa17ba3488",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['caress',\n",
       " 'fly',\n",
       " 'dy',\n",
       " 'mule',\n",
       " 'denied',\n",
       " 'died',\n",
       " 'agreed',\n",
       " 'owned',\n",
       " 'humbled',\n",
       " 'sized',\n",
       " 'meeting',\n",
       " 'stating',\n",
       " 'siezing',\n",
       " 'itemization',\n",
       " 'sensational',\n",
       " 'traditional',\n",
       " 'reference',\n",
       " 'colonizer',\n",
       " 'plotted']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[ lemmatizer.lemmatize(word) for word in words]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f022f4b-5866-4852-a18d-d86dfb0ff9a5",
   "metadata": {},
   "source": [
    "##### Lemmatize using Spacy\n",
    "spaCy considers context (so is presumably better on the whole)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b344bd7c-626c-48e7-8d64-1391435263ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(' '.join(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "55627dd8-6077-438b-a710-7f9ccdfdb10c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['caress',\n",
       " 'fly',\n",
       " 'die',\n",
       " 'mule',\n",
       " 'deny',\n",
       " 'died',\n",
       " 'agree',\n",
       " 'own',\n",
       " 'humble',\n",
       " 'sized',\n",
       " 'meeting',\n",
       " 'state',\n",
       " 'sieze',\n",
       " 'itemization',\n",
       " 'sensational',\n",
       " 'traditional',\n",
       " 'reference',\n",
       " 'colonizer',\n",
       " 'plot']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[token.lemma_ for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a2dc02-c466-47fa-8a4e-acb0039d95ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

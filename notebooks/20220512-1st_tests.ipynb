{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbb2b8b8-1fca-4987-9a83-e2f2a62f697d",
   "metadata": {},
   "source": [
    "#### This is the first notebook for working through Real-World NLP\n",
    "\n",
    "Hagiwara, Masato. Real-World Natural Language Processing (p. 83). Manning. Kindle Edition. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0636d95-6a0c-4890-a6f1-1b1f9117fe15",
   "metadata": {},
   "source": [
    "Everything in the cell below ran in my python 3.9.7 poetry environment :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13b48cda-a8f0-439f-a42c-7ce0f068ab51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "from typing import Dict\n",
    " \n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from allennlp.data.data_loaders import MultiProcessDataLoader\n",
    "from allennlp.data.samplers import BucketBatchSampler\n",
    "from allennlp.data.vocabulary import Vocabulary\n",
    "from allennlp.models import Model\n",
    "from allennlp.modules.seq2vec_encoders import Seq2VecEncoder, PytorchSeq2VecWrapper\n",
    "from allennlp.modules.text_field_embedders import TextFieldEmbedder, BasicTextFieldEmbedder\n",
    "from allennlp.modules.token_embedders import Embedding\n",
    "from allennlp.nn.util import get_text_field_mask\n",
    "from allennlp.training import GradientDescentTrainer\n",
    "from allennlp.training.metrics import CategoricalAccuracy, F1Measure\n",
    "from allennlp_models.classification.dataset_readers.stanford_sentiment_tree_bank import StanfordSentimentTreeBankDatasetReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "005d9f8d-96a6-4ecd-8d28-d4e6324d8aca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 128\n",
    "HIDDEN_DIM = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22034f4c-6292-48d0-9365-34b9ab50e899",
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = StanfordSentimentTreeBankDatasetReader()\n",
    "train_path = 'https://s3.amazonaws.com/realworldnlpbook/data/stanfordSentimentTreebank/trees/train.txt'\n",
    "dev_path = 'https://s3.amazonaws.com/realworldnlpbook/data/stanfordSentimentTreebank/trees/dev.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82316124-cb50-4c53-8152-dd1233ee31b3",
   "metadata": {},
   "source": [
    "That completes the initial setup."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8d890d-dc6e-4aa9-9c67-abf41e3f1121",
   "metadata": {},
   "source": [
    "#### 2.3.2 Using Word Embedding for snteiment analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676000c5-668a-415d-9b89-d5868206fbd9",
   "metadata": {},
   "source": [
    "Create dataset loaders to pass data to training pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e76f4be-79e2-4330-8d32-d122d2fdc14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!poetry run jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735bdb0b-aab2-41c3-87f1-6f776e9438ff",
   "metadata": {},
   "source": [
    "Create dataset loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab2c8a85-5b94-4e18-aea1-615b21caba39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64f07fecb3cb4df19f54af1ba7cb618f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "loading instances: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "961dc47f6f444ad5975c7070a0174fd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "loading instances: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sampler = BucketBatchSampler(batch_size=32, sorting_keys=[\"tokens\"])\n",
    "train_data_loader = MultiProcessDataLoader(reader, train_path, batch_sampler=sampler)\n",
    "dev_data_loader = MultiProcessDataLoader(reader, dev_path, batch_sampler = sampler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73b23c4-deab-4162-bd77-45927a5910e5",
   "metadata": {},
   "source": [
    "Create a vocabularly instance from a set of instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6fd963df-6ead-4fbc-9ad9-ba9c94da5809",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b23520d0f4974c399e47736bc375dca8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "building vocab: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vocab = Vocabulary.from_instances(chain(train_data_loader.iter_instances(), dev_data_loader.iter_instances()), min_count={'tokens':3})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dabdd207-370b-4dba-b5ba-915e267ed7a0",
   "metadata": {},
   "source": [
    "The size of embeddigns is governed by EMBEDDING_DIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "043faf96-2973-4f0a-b700-d858a50673a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_loader.index_with(vocab)\n",
    "dev_data_loader.index_with(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "584bc74b-4d69-4a81-847a-8fd646bf362a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "token_embeddings = Embedding(num_embeddings=vocab.get_vocab_size('tokens'),embedding_dim=EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ca8bb8-9f97-4286-9f8e-7973aa657e72",
   "metadata": {},
   "source": [
    "Specify which index names corrrespond to which bembeddigns and pass it to BasicTextFieldEmbedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14394b82-3f3b-40d9-a34f-a28cc399e2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings = BasicTextFieldEmbedder({'tokens': token_embeddings})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef48a07f-c0e6-4cb1-b5d7-188458d274d8",
   "metadata": {},
   "source": [
    "Create an RNN, specifically an LSTM (Long Short-Term Memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0613b82-29ec-4ac3-be0c-9a0ef85c7bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = PytorchSeq2VecWrapper(torch.nn.LSTM(EMBEDDING_DIM, HIDDEN_DIM, batch_first=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c455017b-a5be-4c48-83f5-6902a7bdad59",
   "metadata": {},
   "source": [
    "Create a linear Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c9db068a-01f6-4641-ab89-69ee35e30d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model in AllenNLP represents a model that is trained.\n",
    "class LstmClassifier(Model):\n",
    "    def __init__(self,\n",
    "                 embedder: TextFieldEmbedder,\n",
    "                 encoder: Seq2VecEncoder,\n",
    "                 vocab: Vocabulary,\n",
    "                 positive_label: str = '4') -> None:\n",
    "        super().__init__(vocab)\n",
    "        # We need the embeddings to convert word IDs to their vector representations\n",
    "        self.embedder = embedder\n",
    "\n",
    "        self.encoder = encoder\n",
    "\n",
    "        # After converting a sequence of vectors to a single vector, we feed it into\n",
    "        # a fully-connected linear layer to reduce the dimension to the total number of labels.\n",
    "        self.linear = torch.nn.Linear(in_features=encoder.get_output_dim(),\n",
    "                                      out_features=vocab.get_vocab_size('labels'))\n",
    "\n",
    "        # Monitor the metrics - we use accuracy, as well as prec, rec, f1 for 4 (very positive)\n",
    "        positive_index = vocab.get_token_index(positive_label, namespace='labels')\n",
    "        self.accuracy = CategoricalAccuracy()\n",
    "        self.f1_measure = F1Measure(positive_index)\n",
    "\n",
    "        # We use the cross entropy loss because this is a classification task.\n",
    "        # Note that PyTorch's CrossEntropyLoss combines softmax and log likelihood loss,\n",
    "        # which makes it unnecessary to add a separate softmax layer.\n",
    "        self.loss_function = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # Instances are fed to forward after batching.\n",
    "    # Fields are passed through arguments with the same name.\n",
    "    def forward(self,\n",
    "                tokens: Dict[str, torch.Tensor],\n",
    "                label: torch.Tensor = None) -> torch.Tensor:\n",
    "        # In deep NLP, when sequences of tensors in different lengths are batched together,\n",
    "        # shorter sequences get padded with zeros to make them equal length.\n",
    "        # Masking is the process to ignore extra zeros added by padding\n",
    "        mask = get_text_field_mask(tokens)\n",
    "\n",
    "        # Forward pass\n",
    "        embeddings = self.embedder(tokens)\n",
    "        encoder_out = self.encoder(embeddings, mask)\n",
    "        logits = self.linear(encoder_out)\n",
    "\n",
    "        # In AllenNLP, the output of forward() is a dictionary.\n",
    "        # Your output dictionary must contain a \"loss\" key for your model to be trained.\n",
    "        output = {\"logits\": logits}\n",
    "        if label is not None:\n",
    "            self.accuracy(logits, label)\n",
    "            self.f1_measure(logits, label)\n",
    "            output[\"loss\"] = self.loss_function(logits, label)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def get_metrics(self, reset: bool = False) -> Dict[str, float]:\n",
    "        return {'accuracy': self.accuracy.get_metric(reset),\n",
    "                **self.f1_measure.get_metric(reset)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "834e07b0-23b5-4288-83ae-408f8ed54c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e3f721-29f0-498b-8cae-e8b5fa427f2b",
   "metadata": {},
   "source": [
    "Define and initialise model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cba285f3-def7-475d-ab88-251be4426f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LstmClassifier(word_embeddings, encoder, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37ca00e-e880-43cd-8f9d-61c1e8cc6ae1",
   "metadata": {},
   "source": [
    "Allennlp  'Trainer' class acts as a framework for putting sll thr components together and managing the training pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "035b1353-47d8-4aff-8f33-848bf94840c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8c59c670e66466cb6550e29d5245895",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/267 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18edd14404d04ab78a66b96f0c522e09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9ddbd28b54c44c294fc8a14b233bcb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/267 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c3796116f2d4afcbbd63bfd5d31a52a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4855caed6871466887f840bad7031b6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/267 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfec7065f9dd4b4f8d48872ba155ffec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1d1f8bb07274c15852327d17abbaf12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/267 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f23043988a734be8869d3da8f56c4d9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd04cae2a622422d8f6ade7ca4355670",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/267 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c250ad197cde45d4b845013eb88d6a1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef374c88791741e0b92b0267f7257a91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/267 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a03475d094e4623882e2943e52ba853",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ea5a25ef9eb48a08a708d79d35647a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/267 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d8e2d7b59ba47808848f8a9bb6c6340",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "377ef57e01ac464ebdd9ce4038be9217",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/267 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1600a9aac3284111b94483f568f951d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7916abdbb1b049f69371946a21a9cb28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/267 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff33e78e218d41a9afa0ed6eb544ef90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68c6e8e7664f4846aa343be82ea239f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/267 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c0f7aef12414068b3f8b196fbcf9c49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc3ba3b9480b4ef3abfa99353575649b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/267 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5aab59d85d1b4c00b41ddc1bfac26a89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "809e4e6bda2d47978eccc8a868212031",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/267 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8fefa104b0a423aaa03a890e30fcde4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'best_epoch': 1,\n",
       " 'peak_worker_0_memory_MB': 340.74609375,\n",
       " 'training_duration': '0:01:44.733839',\n",
       " 'epoch': 11,\n",
       " 'training_accuracy': 0.9207631086142322,\n",
       " 'training_precision': 0.9501187801361084,\n",
       " 'training_recall': 0.9316770434379578,\n",
       " 'training_f1': 0.9408075213432312,\n",
       " 'training_loss': 0.2790430114510354,\n",
       " 'training_worker_0_memory_MB': 340.74609375,\n",
       " 'validation_accuracy': 0.3678474114441417,\n",
       " 'validation_precision': 0.38372093439102173,\n",
       " 'validation_recall': 0.4000000059604645,\n",
       " 'validation_f1': 0.3916914165019989,\n",
       " 'validation_loss': 2.2758769069399154,\n",
       " 'best_validation_accuracy': 0.368755676657584,\n",
       " 'best_validation_precision': 0.4204545319080353,\n",
       " 'best_validation_recall': 0.4484848380088806,\n",
       " 'best_validation_f1': 0.4340175986289978,\n",
       " 'best_validation_loss': 1.3868441070829118}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = GradientDescentTrainer(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    data_loader=train_data_loader,\n",
    "    validation_data_loader=dev_data_loader,\n",
    "    patience=10,\n",
    "    num_epochs=20,\n",
    "    cuda_device=-1)\n",
    " \n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc7aa6d-9c8e-4c8c-8869-1d53d03c0b49",
   "metadata": {},
   "source": [
    "Allennlp provides a framework called 'predictors' whose job it is to receive input in raw form and (e.g. a string) ,pass it througn pre-processing and the neural network pipeline and return the results.<br>\n",
    "Here Hagiwara has written a predictor for SST called 'SentenceClassifierPredictor'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "be83986e-0d00-487e-b444-a0769f8bf355",
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.common import JsonDict\n",
    "from allennlp.data import DatasetReader, Instance\n",
    "from allennlp.data.tokenizers.spacy_tokenizer import SpacyTokenizer\n",
    "from allennlp.models import Model\n",
    "from allennlp.predictors import Predictor\n",
    "from overrides import overrides\n",
    "from typing import List\n",
    "\n",
    "\n",
    "# You need to name your predictor and register so that `allennlp` command can recognize it\n",
    "# Note that you need to use \"@Predictor.register\", not \"@Model.register\"!\n",
    "@Predictor.register(\"sentence_classifier_predictor\")\n",
    "class SentenceClassifierPredictor(Predictor):\n",
    "    def __init__(self, model: Model, dataset_reader: DatasetReader) -> None:\n",
    "        super().__init__(model, dataset_reader)\n",
    "        self._tokenizer = dataset_reader._tokenizer or SpacyTokenizer()\n",
    "\n",
    "    def predict(self, sentence: str) -> JsonDict:\n",
    "        return self.predict_json({\"sentence\" : sentence})\n",
    "\n",
    "    @overrides\n",
    "    def _json_to_instance(self, json_dict: JsonDict) -> Instance:\n",
    "        sentence = json_dict[\"sentence\"]\n",
    "        tokens = self._tokenizer.tokenize(sentence)\n",
    "        return self._dataset_reader.text_to_instance([str(t) for t in tokens])\n",
    "\n",
    "\n",
    "@Predictor.register(\"universal_pos_predictor\")\n",
    "class UniversalPOSPredictor(Predictor):\n",
    "    def __init__(self, model: Model, dataset_reader: DatasetReader) -> None:\n",
    "        super().__init__(model, dataset_reader)\n",
    "\n",
    "    def predict(self, words: List[str]) -> JsonDict:\n",
    "        return self.predict_json({\"words\" : words})\n",
    "\n",
    "    @overrides\n",
    "    def _json_to_instance(self, json_dict: JsonDict) -> Instance:\n",
    "        words = json_dict[\"words\"]\n",
    "        # This is a hack - the second argument to text_to_instance is a list of POS tags\n",
    "        # that has the same length as words. We don't need it for prediction so\n",
    "        # just pass words.\n",
    "        return self._dataset_reader.text_to_instance(words, words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6c5a468d-91c5-4169-a1ff-a6d872f36f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = SentenceClassifierPredictor(model, dataset_reader=reader)\n",
    "logits = predictor.predict('This is the best movie ever!')['logits']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "286e5b39-3be2-4b9e-98b4-372023f04cb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.3762405216693878,\n",
       " -0.6694225072860718,\n",
       " -0.47617673873901367,\n",
       " 1.10770583152771,\n",
       " -0.13831138610839844]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883735cb-6269-44da-a499-90e2d46ea251",
   "metadata": {},
   "source": [
    "Note the predictor returns the raw output from the model, in this case 'logits'<br>\n",
    "(Remember, logits are a kind of score corresponding to target labels).<br>\n",
    "If you want the predicted albels themselves you need to convert it, in this case it can be done as follows/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1ebdb13a-e984-40a4-8e17-e009ef148870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "label_id = np.argmax(logits)\n",
    "print(model.vocab.get_token_from_index(label_id, 'labels'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704e213c-4b60-424a-a9e7-20022333cfc0",
   "metadata": {},
   "source": [
    "If this prints out a “4,” congratulations! Label “4” corresponds to “very positive,” so your sentiment analyzer just predicted that the sentence “This is the best movie ever!” is very positive, which is indeed correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb9a492-82f4-4627-a127-deff66957f44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
